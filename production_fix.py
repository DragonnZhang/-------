#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç”Ÿäº§ç¯å¢ƒæ‰¹é‡ä¿®å¤è„šæœ¬
ä½¿ç”¨ç»è¿‡éªŒè¯çš„åçˆ¬è™«è§£å†³æ–¹æ¡ˆ
"""

import json
import time
import random
from pathlib import Path
from datetime import datetime
import logging

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('production_fix.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# ä»practical_crawlerå¯¼å…¥æ ¸å¿ƒåŠŸèƒ½
from practical_crawler import PracticalCrawler

def get_problematic_articles():
    """è·å–æ‰€æœ‰é—®é¢˜æ–‡ç« åˆ—è¡¨"""
    articles_dir = Path("articles")
    problematic = []
    
    print("æ­£åœ¨æ‰«æé—®é¢˜æ–‡ç« ...")
    
    for date_dir in articles_dir.iterdir():
        if not date_dir.is_dir():
            continue
            
        for article_file in date_dir.glob("*.json"):
            try:
                with open(article_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    
                content = data.get('content', {})
                title = content.get('title', '')
                article_content = content.get('content', '')
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯é—®é¢˜æ–‡ç« 
                is_problematic = (
                    title == '491 Forbidden' or
                    article_content == 'æ— å†…å®¹' or
                    '491 Forbidden' in title or
                    len(article_content) < 50
                )
                
                if is_problematic:
                    metadata = data.get('metadata', {})
                    
                    # ä»æ–‡ä»¶åæ¨æ–­é¡µç 
                    filename = article_file.stem
                    parts = filename.split('_')
                    page_no = parts[1] if len(parts) >= 3 else '001'
                    
                    problematic.append({
                        'date': date_dir.name,
                        'page_no': page_no,
                        'metadata': metadata,
                        'file_path': article_file,
                        'title': metadata.get('mainTitle', 'æœªçŸ¥æ ‡é¢˜')[:50] + "..."
                    })
                    
            except Exception as e:
                logger.error(f"è¯»å–æ–‡ä»¶å¤±è´¥ {article_file}: {e}")
    
    return problematic

def production_batch_fix():
    """ç”Ÿäº§ç¯å¢ƒæ‰¹é‡ä¿®å¤"""
    print("="*60)
    print("äººæ°‘é‚®ç”µæŠ¥çˆ¬è™« - ç”Ÿäº§ç¯å¢ƒæ‰¹é‡ä¿®å¤å·¥å…·")
    print("="*60)
    
    # è·å–é—®é¢˜æ–‡ç« 
    problematic_articles = get_problematic_articles()
    
    if not problematic_articles:
        print("ğŸ‰ æ²¡æœ‰å‘ç°é—®é¢˜æ–‡ç« ï¼æ‰€æœ‰æ–‡ç« éƒ½å·²æˆåŠŸçˆ¬å–ã€‚")
        return
    
    total_articles = len(problematic_articles)
    print(f"\\nğŸ“Š å‘ç° {total_articles} ç¯‡é—®é¢˜æ–‡ç« éœ€è¦ä¿®å¤")
    
    # æ˜¾ç¤ºå‰10ç¯‡æ–‡ç« 
    print("\\nğŸ“‹ å‰10ç¯‡é—®é¢˜æ–‡ç« :")
    for i, article in enumerate(problematic_articles[:10], 1):
        print(f"{i:2d}. {article['date']}: {article['title']}")
    
    if total_articles > 10:
        print(f"    ... è¿˜æœ‰ {total_articles - 10} ç¯‡")
    
    # è®¾ç½®æ‰¹æ¬¡å¤§å°
    batch_size = 5
    print(f"\\nâš™ï¸ é…ç½®:")
    print(f"   - æ‰¹æ¬¡å¤§å°: {batch_size} ç¯‡/æ‰¹æ¬¡")
    print(f"   - æ–‡ç« é—´å»¶è¿Ÿ: 60ç§’ (1åˆ†é’Ÿ)")
    print(f"   - æ‰¹æ¬¡é—´å»¶è¿Ÿ: 120-240ç§’ (2-4åˆ†é’Ÿ)")
    print(f"   - é¢„è®¡æ€»æ—¶é—´: {total_articles * 60 / 60:.0f}-{total_articles * 80 / 60:.0f} åˆ†é’Ÿ")
    
    # ç¡®è®¤æ‰§è¡Œ
    print("\\nâš ï¸  æ³¨æ„äº‹é¡¹:")
    print("   - è¯·ç¡®ä¿ç½‘ç»œè¿æ¥ç¨³å®š")
    print("   - è¿‡ç¨‹ä¸­è¯·å‹¿å…³é—­ç¨‹åº")
    print("   - å¦‚é‡åˆ°è¿ç»­å¤±è´¥ï¼Œç¨‹åºä¼šè‡ªåŠ¨æš‚åœ")
    
    response = input(f"\\nğŸš€ æ˜¯å¦å¼€å§‹ä¿®å¤ï¼Ÿ(y/n): ")
    if response.lower() != 'y':
        print("âŒ æ“ä½œå·²å–æ¶ˆ")
        return
    
    # å¼€å§‹ä¿®å¤
    print(f"\\nğŸ”§ å¼€å§‹æ‰¹é‡ä¿®å¤ ({datetime.now().strftime('%Y-%m-%d %H:%M:%S')})")
    print("-" * 60)
    
    crawler = PracticalCrawler()
    success_count = 0
    fail_count = 0
    consecutive_fails = 0
    
    for i, article in enumerate(problematic_articles, 1):
        print(f"\\n[{i}/{total_articles}] ä¿®å¤æ–‡ç« :")
        print(f"ğŸ“… æ—¥æœŸ: {article['date']}")
        print(f"ğŸ“° æ ‡é¢˜: {article['title']}")
        
        try:
            success = crawler.fix_single_article(
                article['date'],
                article['page_no'],
                article['metadata']
            )
            
            if success:
                success_count += 1
                consecutive_fails = 0
                print(f"âœ… ä¿®å¤æˆåŠŸ (æˆåŠŸç‡: {success_count}/{i} = {success_count/i*100:.1f}%)")
            else:
                fail_count += 1
                consecutive_fails += 1
                print(f"âŒ ä¿®å¤å¤±è´¥ (è¿ç»­å¤±è´¥: {consecutive_fails})")
                
                # è¿ç»­å¤±è´¥ä¿æŠ¤
                if consecutive_fails >= 3:
                    print("\\nâš ï¸ è¿ç»­3æ¬¡å¤±è´¥ï¼Œå¯èƒ½é‡åˆ°æ›´ä¸¥æ ¼çš„åçˆ¬è™«æœºåˆ¶")
                    print("å»ºè®®ï¼š")
                    print("1. æš‚åœ30-60åˆ†é’Ÿåå†æ¬¡å°è¯•")
                    print("2. æˆ–è€…å¢åŠ å»¶è¿Ÿæ—¶é—´")
                    
                    choice = input("æ˜¯å¦ç»§ç»­ï¼Ÿ(y/n): ")
                    if choice.lower() != 'y':
                        break
                    consecutive_fails = 0
                    
        except Exception as e:
            fail_count += 1
            consecutive_fails += 1
            print(f"âŒ ä¿®å¤å‡ºé”™: {e}")
            logger.error(f"ä¿®å¤æ–‡ç« å‡ºé”™: {e}")
        
        # è¿›åº¦æŠ¥å‘Š
        if i % batch_size == 0 and i < total_articles:
            print(f"\\nğŸ“Š æ‰¹æ¬¡å®Œæˆ: {i}/{total_articles}")
            print(f"ğŸ“ˆ å½“å‰æˆåŠŸç‡: {success_count}/{i} = {success_count/i*100:.1f}%")
            
            # æ‰¹æ¬¡é—´æ›´é•¿å»¶è¿Ÿ
            batch_delay = random.uniform(60, 120)
            print(f"â³ æ‰¹æ¬¡é—´ä¼‘æ¯ {batch_delay:.0f}s...")
            time.sleep(batch_delay)
    
    # æœ€ç»ˆæŠ¥å‘Š
    print("\\n" + "="*60)
    print("ğŸ æ‰¹é‡ä¿®å¤å®Œæˆ")
    print("="*60)
    print(f"ğŸ“Š æ€»ä½“ç»Ÿè®¡:")
    print(f"   - å¤„ç†æ–‡ç« : {i}")
    print(f"   - ä¿®å¤æˆåŠŸ: {success_count}")
    print(f"   - ä¿®å¤å¤±è´¥: {fail_count}")
    print(f"   - æˆåŠŸç‡: {success_count/i*100:.1f}%")
    
    if success_count > 0:
        print(f"\\nâœ… å»ºè®®ä¸‹ä¸€æ­¥:")
        print(f"   1. è¿è¡Œ python check_status.py æ£€æŸ¥æœ€æ–°çŠ¶æ€")
        print(f"   2. å¦‚è¿˜æœ‰æ–‡ç« éœ€è¦ä¿®å¤ï¼Œç­‰å¾…1-2å°æ—¶åå†æ¬¡è¿è¡Œ")
        print(f"   3. æ£€æŸ¥ä¿®å¤åçš„æ–‡ç« å†…å®¹è´¨é‡")
    
    print(f"\\nğŸ“ è¯¦ç»†æ—¥å¿—å·²ä¿å­˜åˆ°: production_fix.log")
    print(f"ğŸ• å®Œæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

if __name__ == "__main__":
    production_batch_fix()
